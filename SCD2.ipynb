{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988439e8-f2d2-4f12-b8ed-12c5983a20a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_date, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, BooleanType, DoubleType\n",
    "from datetime import date\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SCD2_Example\").getOrCreate()\n",
    "\n",
    "# Define Schemas\n",
    "provider_schema = StructType([\n",
    "    StructField(\"provider_key\", IntegerType(), False),\n",
    "    StructField(\"provider_id\", StringType(), False),\n",
    "    StructField(\"provider_name\", StringType(), True),\n",
    "    StructField(\"specialty\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"start_date\", DateType(), False),\n",
    "    StructField(\"end_date\", DateType(), True),\n",
    "    StructField(\"is_current\", BooleanType(), False)\n",
    "])\n",
    "\n",
    "claims_schema = StructType([\n",
    "    StructField(\"claim_id\", StringType(), False),\n",
    "    StructField(\"member_id\", StringType(), False),\n",
    "    StructField(\"claim_date\", DateType(), False),\n",
    "    StructField(\"claim_amount\", DoubleType(), True),\n",
    "    StructField(\"provider_key\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Initial Provider Data\n",
    "initial_provider_data = [\n",
    "    (1, 'P101', 'Dr. Smith', 'Cardiology', 'NY', date(2020, 1, 1), None, True),\n",
    "    (2, 'P102', 'Dr. Jones', 'Pediatrics', 'CA', date(2019, 5, 15), None, True)\n",
    "]\n",
    "\n",
    "# Create DataFrame and temporary view\n",
    "dim_provider = spark.createDataFrame(initial_provider_data, schema=provider_schema)\n",
    "dim_provider.createOrReplaceTempView(\"dim_provider_view\")\n",
    "\n",
    "# Create an empty claims table for now\n",
    "fct_claims = spark.createDataFrame([], schema=claims_schema)\n",
    "fct_claims.createOrReplaceTempView(\"fct_claims_view\")\n",
    "\n",
    "print(\"Initial Provider Dimension:\")\n",
    "display(dim_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a6a0e46-afcb-492c-805a-f411e70346d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, BooleanType\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"provider_key\", IntegerType(), True),\n",
    "    StructField(\"provider_id\", StringType(), True),\n",
    "    StructField(\"provider_name\", StringType(), True),\n",
    "    StructField(\"specialty\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"start_date\", DateType(), True),\n",
    "    StructField(\"end_date\", DateType(), True),\n",
    "    StructField(\"is_current\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# New data representing the change for Dr. Smith\n",
    "update_data = [(3, 'P101', 'Dr. Smith', 'Cardiology', 'CA', datetime.strptime('2024-06-15', '%Y-%m-%d').date(), None, True)]\n",
    "\n",
    "# Create DataFrame with the specified schema\n",
    "updates_df = spark.createDataFrame(update_data, schema)\n",
    "updates_df.createOrReplaceTempView(\"provider_updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52985fdf-5b2b-47a3-9804-cbc25cdbe299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def implement_provider_scd1(target_table, source_df, key_column=\"provider_id\"):\n",
    "    \"\"\"\n",
    "    Implement SCD Type 1 for provider table - practice location changes\n",
    "    \n",
    "    This overwrites:\n",
    "    - Practice location changes (current location is sufficient)\n",
    "    - Contact information updates  \n",
    "    - Administrative corrections\n",
    "    \n",
    "    This preserves:\n",
    "    - Provider specialty (might need history)\n",
    "    - PCP status (might need history)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== PROVIDER SCD TYPE 1 IMPLEMENTATION ===\")\n",
    "    \n",
    "    # Read current provider table\n",
    "    current_providers = spark.table(target_table)\n",
    "    print(\"Current provider table:\")\n",
    "    current_providers.show()\n",
    "    \n",
    "    print(\"Provider location changes to apply:\")\n",
    "    source_df.show()\n",
    "    \n",
    "    # Identify what columns to update (SCD Type 1 columns)\n",
    "    scd1_columns = [\"practice_location\", \"name\"]  # Location and name corrections\n",
    "    \n",
    "    # Show before state\n",
    "    print(\"=== BEFORE CHANGES ===\")\n",
    "    before_changes = current_providers.filter(\n",
    "        col(key_column).isin([row[key_column] for row in source_df.collect()])\n",
    "    )\n",
    "    before_changes.show()\n",
    "    \n",
    "    # For SCD Type 1, we simply merge/upsert the changes\n",
    "    # Get records that are not being updated\n",
    "    unchanged_providers = current_providers.join(\n",
    "        source_df.select(key_column),\n",
    "        key_column,\n",
    "        \"left_anti\"\n",
    "    )\n",
    "    \n",
    "    # Combine unchanged records with all source records (updated + new)\n",
    "    updated_providers = unchanged_providers.union(source_df)\n",
    "    \n",
    "    # Alternative approach using Delta merge (if available)\n",
    "    try:\n",
    "        from delta.tables import DeltaTable\n",
    "        \n",
    "        # Try Delta Lake merge for better performance\n",
    "        delta_table = DeltaTable.forName(spark, target_table)\n",
    "        \n",
    "        # Build update dictionary for SCD1 columns\n",
    "        update_dict = {col: f\"source.{col}\" for col in scd1_columns}\n",
    "        update_dict[\"Provider_Plan_ID\"] = \"source.Provider_Plan_ID\"\n",
    "        update_dict[\"specialty\"] = \"source.specialty\"\n",
    "        update_dict[\"is_pcp\"] = \"source.is_pcp\"\n",
    "        update_dict[\"ID\"] = \"source.ID\"\n",
    "        update_dict[\"TaxID\"] = \"source.TaxID\"\n",
    "        \n",
    "        delta_table.alias(\"target\") \\\n",
    "            .merge(\n",
    "                source_df.alias(\"source\"),\n",
    "                f\"target.{key_column} = source.{key_column}\"\n",
    "            ) \\\n",
    "            .whenMatchedUpdate(set=update_dict) \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n",
    "        \n",
    "        print(\"Used Delta Lake merge for SCD Type 1\")\n",
    "        result_df = spark.table(target_table)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Delta merge not available, using DataFrame operations: {e}\")\n",
    "        \n",
    "        # Save using DataFrame operations\n",
    "        updated_providers.write.mode(\"overwrite\").saveAsTable(f\"{target_table}_scd1_updated\")\n",
    "        result_df = spark.table(f\"{target_table}_scd1_updated\")\n",
    "    \n",
    "    # Show after state\n",
    "    print(\"=== AFTER CHANGES ===\")\n",
    "    after_changes = result_df.filter(\n",
    "        col(key_column).isin([row[key_column] for row in source_df.collect()])\n",
    "    )\n",
    "    after_changes.show()\n",
    "    \n",
    "    # Show change summary\n",
    "    print(\"=== CHANGE SUMMARY ===\")\n",
    "    total_providers = result_df.count()\n",
    "    updated_count = source_df.count()\n",
    "    new_providers = source_df.join(current_providers.select(key_column), key_column, \"left_anti\").count()\n",
    "    \n",
    "    print(f\"Total providers after update: {total_providers}\")\n",
    "    print(f\"Records processed: {updated_count}\")\n",
    "    print(f\"New providers added: {new_providers}\")\n",
    "    print(f\"Existing providers updated: {updated_count - new_providers}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Create realistic provider location changes\n",
    "provider_changes = [\n",
    "    # Dr. James Wilson moved to new location\n",
    "    (2001, \"Dr. James Wilson\", \"Family Medicine\", \"Downtown Medical Center\", 1, \"PR0001\", 1, 957974335),\n",
    "    \n",
    "    # Dr. Sarah Johnson name correction and location change  \n",
    "    (2002, \"Dr. Sarah M. Johnson\", \"Pediatrics\", \"Children's Hospital East Wing\", 1, \"PR0002\", 2, 123456789),\n",
    "    \n",
    "    # Existing provider location change\n",
    "    (2003, \"Dr. Michael Chen\", \"Cardiology\", \"Heart & Vascular Institute\", 0, \"PR0003\", 3, 987654321),\n",
    "    \n",
    "    # New provider joining the network\n",
    "    (2004, \"Dr. Lisa Rodriguez\", \"Orthopedics\", \"Sports Medicine Center\", 0, \"PR0004\", 4, 555666777),\n",
    "    \n",
    "    # Provider location consolidation\n",
    "    (2005, \"Dr. Robert Kim\", \"Internal Medicine\", \"North Clinic\", 1, \"PR0005\", 5, 111222333)\n",
    "]\n",
    "\n",
    "provider_schema = StructType([\n",
    "    StructField(\"Provider_Plan_ID\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"specialty\", StringType(), True),\n",
    "    StructField(\"practice_location\", StringType(), True),\n",
    "    StructField(\"is_pcp\", IntegerType(), True),\n",
    "    StructField(\"provider_id\", StringType(), True),\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"TaxID\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "provider_updates = spark.createDataFrame(provider_changes, provider_schema)\n",
    "\n",
    "print(\"=== PROVIDER LOCATION CHANGES TO PROCESS ===\")\n",
    "provider_updates.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "849904db-8a16-4784-8bd7-8a3c91efc0e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze provider location changes over time\n",
    "def analyze_provider_changes():\n",
    "    \"\"\"\n",
    "    Analyze patterns in provider location changes\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== PROVIDER CHANGE ANALYTICS ===\")\n",
    "    \n",
    "    # Group providers by location to see consolidation patterns\n",
    "    location_analysis = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            practice_location,\n",
    "            COUNT(*) as provider_count,\n",
    "            COUNT(CASE WHEN is_pcp = 1 THEN 1 END) as pcp_count,\n",
    "            COLLECT_LIST(specialty) as specialties\n",
    "        FROM default.providers\n",
    "        GROUP BY practice_location\n",
    "        ORDER BY provider_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"Provider distribution by location:\")\n",
    "    location_analysis.show(truncate=False)\n",
    "    \n",
    "    # Specialty distribution\n",
    "    specialty_analysis = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            specialty,\n",
    "            COUNT(*) as provider_count,\n",
    "            COUNT(CASE WHEN is_pcp = 1 THEN 1 END) as pcp_count,\n",
    "            COUNT(DISTINCT practice_location) as locations\n",
    "        FROM default.providers\n",
    "        GROUP BY specialty\n",
    "        ORDER BY provider_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"Provider distribution by specialty:\")\n",
    "    specialty_analysis.show()\n",
    "    \n",
    "    return location_analysis, specialty_analysis\n",
    "\n",
    "# Run provider analytics\n",
    "location_stats, specialty_stats = analyze_provider_changes()\n",
    "\n",
    "# Validate provider data quality after SCD1 updates\n",
    "def validate_provider_data_quality():\n",
    "    \"\"\"\n",
    "    Validate provider data quality after updates\n",
    "    \"\"\"\n",
    "    print(\"=== PROVIDER DATA QUALITY VALIDATION ===\")\n",
    "    \n",
    "    providers_df = spark.table(\"default.providers\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicate_providers = providers_df.groupBy(\"provider_id\").count().filter(col(\"count\") > 1)\n",
    "    print(f\"Duplicate provider IDs: {duplicate_providers.count()}\")\n",
    "    \n",
    "    # Check for missing required fields\n",
    "    missing_data = providers_df.filter(\n",
    "        col(\"name\").isNull() | \n",
    "        col(\"specialty\").isNull() | \n",
    "        col(\"practice_location\").isNull() |\n",
    "        col(\"provider_id\").isNull()\n",
    "    )\n",
    "    print(f\"Records with missing required data: {missing_data.count()}\")\n",
    "    \n",
    "    # Check PCP distribution\n",
    "    pcp_distribution = providers_df.groupBy(\"is_pcp\").count()\n",
    "    print(\"PCP distribution:\")\n",
    "    pcp_distribution.show()\n",
    "    \n",
    "    if missing_data.count() > 0:\n",
    "        print(\"Records with missing data:\")\n",
    "        missing_data.show()\n",
    "\n",
    "# Run validation\n",
    "validate_provider_data_quality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34b52555-0352-4b72-ba4f-4ec2bf306386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, sum, avg, countDistinct\n",
    "\n",
    "def analyze_claims_with_scd_dimensions():\n",
    "    \"\"\"\n",
    "    Demonstrate how to use SCD2 eligibility and SCD1 provider data for claims analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== CLAIMS ANALYSIS WITH SCD DIMENSIONS ===\")\n",
    "    \n",
    "    # Join claims with current eligibility (SCD2) and current provider info (SCD1)\n",
    "    claims_with_dimensions = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            c.claim_id,\n",
    "            c.member_id,\n",
    "            try_cast(c.billed_amount AS DOUBLE) as billed_amount,\n",
    "            try_cast(c.paid_amount AS DOUBLE) as paid_amount,\n",
    "            c.date_of_service,\n",
    "            c.Type_of_Service,\n",
    "            \n",
    "            -- Current eligibility status (SCD2)\n",
    "            e.eligibility_status,\n",
    "            e.eligibility_start_date,\n",
    "            e.eligibility_end_date,\n",
    "            \n",
    "            -- Current provider info (SCD1)\n",
    "            p.name as provider_name,\n",
    "            p.specialty as provider_specialty,\n",
    "            p.practice_location,\n",
    "            p.is_pcp\n",
    "            \n",
    "        FROM default.claims c\n",
    "        \n",
    "        -- Join with current eligibility (SCD2 - point in time)\n",
    "        LEFT JOIN healthanalytics.eligibility e\n",
    "            ON c.member_id = try_cast(e.member_id AS STRING)\n",
    "            AND c.date_of_service >= e.eligibility_start_date\n",
    "            AND c.date_of_service <= e.eligibility_end_date\n",
    "        \n",
    "        -- Join with current provider info (SCD1 - latest version)\n",
    "        LEFT JOIN default.providers p\n",
    "            ON c.ProviderID = p.provider_id\n",
    "        \n",
    "        WHERE c.date_of_service >= '2024-01-01'\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"Claims with SCD dimension data:\")\n",
    "    display(claims_with_dimensions)\n",
    "    \n",
    "    # Analyze claims by eligibility status\n",
    "    eligibility_impact = claims_with_dimensions.groupBy(\"eligibility_status\", \"Type_of_Service\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"claim_count\"),\n",
    "            sum(\"billed_amount\").alias(\"total_billed\"),\n",
    "            avg(\"billed_amount\").alias(\"avg_billed\")\n",
    "        ).orderBy(\"eligibility_status\", \"Type_of_Service\")\n",
    "    \n",
    "    print(\"Claims analysis by eligibility status:\")\n",
    "    display(eligibility_impact)\n",
    "    \n",
    "    # Analyze claims by provider location (after SCD1 updates)\n",
    "    location_impact = claims_with_dimensions.groupBy(\"practice_location\", \"provider_specialty\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"claim_count\"),\n",
    "            sum(\"billed_amount\").alias(\"total_billed\"),\n",
    "            countDistinct(\"member_id\").alias(\"unique_patients\")\n",
    "        ).orderBy(\"total_billed\", ascending=False)\n",
    "    \n",
    "    print(\"Claims analysis by provider location:\")\n",
    "    display(location_impact)\n",
    "    \n",
    "    return claims_with_dimensions\n",
    "\n",
    "# Run integrated analysis\n",
    "integrated_analysis = analyze_claims_with_scd_dimensions()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7741859398679088,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
